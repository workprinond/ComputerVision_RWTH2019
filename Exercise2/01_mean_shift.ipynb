{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean-Shift Clustering and Segmentation\n",
    "\n",
    "In the first part of this task you will implement the *mean-shift* clustering algorithm in a general way (not specifically for anything to do with images, just simply for n-dimensional data points).\n",
    "\n",
    "Then you will apply mean-shift for image segmentation, by clustering data points that represent pixels (e.g. the colors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- Add heading numbers -->\n",
    "<style>\n",
    "body {counter-reset: section;}\n",
    "h2:before {counter-increment: section;\n",
    "           content: counter(section) \" \";}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import imageio\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recap from the lecture\n",
    "The *mean-shift* algorithm clusters an $n$-dimensional data set (i.e., each data point is described by a feature vector of $n$ values) by associating each point with a peak in the estimated probability density of the dataset's distribution. Points associated with the \"same\" peak (up to some small threshold) become members of the same cluster.\n",
    "\n",
    "For each point, mean-shift finds the associated peak by first defining a spherical window of radius $r$ centered on that point, and computing the **mean** of the points that lie within the window. The algorithm then **shifts** the window to be centered on that mean and repeats these steps until convergence, i.e., until the shift is smaller than a specified threshold (set this as 0.1). At each iteration the window shifts to a more densely populated portion of the data set until a peak is reached, where the data is approximately equally distributed in the window.\n",
    "\n",
    "## Finding a peak from a query point\n",
    "Implement the peak searching process as the function `find_peak(data, query, radius)` where\n",
    "\n",
    " * `data` is a $p \\times n$ matrix containing $p$ data points; each point is defined by an $n$-dimensional row vector of feature values\n",
    " * `query` is the $n$-dimensional starting point from which we wish to compute a density peak\n",
    " * `radius` is the search window radius.\n",
    "\n",
    "Return the peak as an $n$-dimensional vector.\n",
    "\n",
    "**Hints:** You can use `np.linalg.norm` to compute the Euclidean norm of a vector. You can also index NumPy arrays with Boolean arrays, e.g. to select only rows that fulfil some criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-456db7ce2d4ca848",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_peak(data, query, radius):\n",
    "    ### BEGIN SOLUTION\n",
    "    while True:\n",
    "        distances = np.linalg.norm(data - query, axis=1)\n",
    "        new_query = np.mean(data[distances < radius], axis=0)\n",
    "        if np.linalg.norm(new_query - query) < 0.1:\n",
    "            return query\n",
    "        query = new_query\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the synthetic dataset `gaussian_mixture_samples_3d.csv` to test your implementation. The data points in this file are 2000 samples from two 3D Gaussian distributions. The following plots only show the projection on the XY plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt(f'gaussian_mixture_samples_3d.csv', delimiter=',')\n",
    "queries = data[[0, 5, 1500]]\n",
    "radius = 2\n",
    "\n",
    "fig, axes = plt.subplots(1, len(queries), figsize=(12,4))\n",
    "for query, ax in zip(queries, axes):\n",
    "    peak = find_peak(data, query, radius)\n",
    "    print('Found peak', peak)\n",
    "    ax.scatter(data[:, 0], data[:, 1], marker='.', color='gray')\n",
    "    ax.scatter(query[0], query[1], s=150, linewidth=5,\n",
    "               color='blue', marker='x', label='starting point')\n",
    "    ax.scatter(peak[0], peak[1], color='orange', marker='x',\n",
    "               s=150, linewidth=5, label='found peak')\n",
    "    ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering all points\n",
    "Implement `mean_shift(data, radius)`, which calls `find_peak` for each point and then assigns a label to each point according to its peak.\n",
    "`mean_shift` should return two arrays: `peaks` and `labels`.\n",
    "\n",
    "  * `peaks` is a matrix with $k$ rows, storing the found density peaks, where $k$ is the data-dependent number of clusters found. \n",
    "  * `labels` is a $p$-sized vector that has an entry for each data point, storing its associated cluster label (an integer)\n",
    "\n",
    "Similar peaks within a distance of `radius`/2 should be considered the same and should be merged after each call to `find_peak`. More specifically, if the peak computed for a data point already exists in `peaks` (according to the distance threshold), then discard the newly computed peak and give the label of the already existing peak to the considered data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-350b0f2a0fb1876f",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mean_shift(data, radius):\n",
    "    labels = np.full(len(data), fill_value=-1, dtype=int)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    # You can also look at the (very slow) implementation in scikit-learn\n",
    "    # https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/cluster/mean_shift_.py\n",
    "    # Try that implementation by uncommenting the following two lines:\n",
    "    # import sklearn.cluster\n",
    "    # return sklearn.cluster.mean_shift(data, radius)\n",
    "    ###\n",
    "    peaks = np.empty(data.shape)\n",
    "    n_peaks = 0\n",
    "    \n",
    "    for idx, query in enumerate(data):\n",
    "        peak = find_peak(data, query, radius)\n",
    "        label = None\n",
    "        # Compare found peak to existing peaks\n",
    "        if n_peaks > 0:\n",
    "            dist = np.linalg.norm(peaks[:n_peaks] - peak, axis=1)\n",
    "            label_of_nearest_peak = np.argmin(dist)\n",
    "            \n",
    "            # If the nearest existing peak is near enough, take its label\n",
    "            if dist[label_of_nearest_peak] < radius / 2:\n",
    "                label = label_of_nearest_peak\n",
    "        \n",
    "        # No existing peak was near enough, create new one\n",
    "        if label is None:\n",
    "            label = n_peaks\n",
    "            peaks[label] = peak\n",
    "            n_peaks += 1\n",
    "        \n",
    "        labels[idx] = label\n",
    "    \n",
    "    peaks = peaks[:n_peaks]\n",
    "    ### END SOLUTION\n",
    "    return peaks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the result of your implementation. The found peaks (cluster centers) are shown as black X marks. You can rotate the interactive 3D plots with the mouse (but it may be somewhat slow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_clusters(ax, data, labels, peaks, \n",
    "                     peak_colors=None, colors=None, axis_names='xyz'):\n",
    "    \"\"\"Plots a set of point clusters in 3D, each with different color.\"\"\"\n",
    "\n",
    "    def luv2rgb(color):\n",
    "        expanded = color[np.newaxis, np.newaxis]\n",
    "        rgb = cv2.cvtColor(expanded.astype(np.uint8), cv2.COLOR_LUV2RGB)\n",
    "        return rgb[0,0]/255\n",
    "      \n",
    "    if peak_colors is None:\n",
    "        peak_colors = peaks\n",
    "    \n",
    "    for label in range(len(peaks)):\n",
    "        if colors=='rgb':\n",
    "            cluster_color = color = peak_colors[label]/255\n",
    "        elif colors=='luv':\n",
    "            cluster_color = luv2rgb(peak_colors[label])\n",
    "        else:\n",
    "            cluster_color=None\n",
    "\n",
    "        cluster = data[labels==label]\n",
    "        ax.scatter(cluster[:, 0], cluster[:, 1], cluster[:, 2],\n",
    "                   alpha=0.15, color=cluster_color)\n",
    "        ax.scatter(peaks[label, 0], peaks[label, 1], peaks[label, 2], \n",
    "                   color='black', marker='x', s=150, linewidth=3)\n",
    "\n",
    "    ax.set_xlabel(axis_names[0])\n",
    "    ax.set_ylabel(axis_names[1])\n",
    "    ax.set_zlabel(axis_names[2])\n",
    "\n",
    "data = np.genfromtxt(f'gaussian_mixture_samples_3d.csv', delimiter=',')\n",
    "radii = [1, 1.25, 2, 8]\n",
    "fig, axes = plt.subplots(\n",
    "    1, len(radii), figsize=(15,4), subplot_kw={'projection': '3d'})\n",
    "\n",
    "for radius, ax in zip(radii, axes): \n",
    "    start_time = time.time()\n",
    "    peaks, labels = mean_shift(data, radius)\n",
    "    plot_3d_clusters(ax, data, labels, peaks)\n",
    "    duration = time.time()-start_time\n",
    "    ax.set_title(\n",
    "        f'Found {len(peaks)} peaks using radius={radius:.2f}\\n'\n",
    "        f'Computation took {duration:.4f} s\\n')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speedups\n",
    "\n",
    "As described so far, the mean-shift algorithm is too slow to be used for image segmentation where each data point corresponds to a pixel. Therefore, you should incorporate the following two speedups from the lecture into your implementation. \n",
    "\n",
    "**First speedup**: upon finding a new peak, associate each data point within `radius` distance from that peak with the cluster defined by that peak. This speedup is known as the *“basin of attraction”* and is based on the intuition that points within one window size distance from the peak will, with high probability, converge to that peak. \n",
    "\n",
    "Call the new function `mean_shift_opt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f0792525823c53d4",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mean_shift_opt(data, radius):\n",
    "    labels = np.full(len(data), fill_value=-1, dtype=int)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    peaks = np.empty(data.shape)\n",
    "    n_peaks = 0\n",
    "    \n",
    "    for idx, query in enumerate(data):\n",
    "        # Skip point if it already got a valid label assigned\n",
    "        if labels[idx] >= 0:\n",
    "            continue\n",
    "            \n",
    "        peak = find_peak(data, query, radius)\n",
    "        label = None\n",
    "        # Compare found peak to existing peaks\n",
    "        if n_peaks > 0:\n",
    "            dist = np.linalg.norm(peaks[:n_peaks] - peak, axis=1)\n",
    "            label_of_nearest_peak = np.argmin(dist)\n",
    "            \n",
    "            # If the nearest existing peak is near enough, take its label\n",
    "            if dist[label_of_nearest_peak] <= radius / 2:\n",
    "                label = label_of_nearest_peak\n",
    "        \n",
    "        # No existing peak was near enough, create new one\n",
    "        if label is None:\n",
    "            label = n_peaks\n",
    "            peaks[label] = peak\n",
    "            n_peaks += 1\n",
    "            \n",
    "            # SPEEDUP 1: give same label to points near the peak\n",
    "            dist = np.linalg.norm(data - peak, axis=1)\n",
    "            labels[dist < radius] = label\n",
    "            \n",
    "        labels[idx] = label\n",
    "    \n",
    "    peaks = peaks[:n_peaks]\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return peaks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now run the code to check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(f'gaussian_mixture_samples_3d.csv', delimiter=',')\n",
    "radii = [1, 1.25, 2, 8]\n",
    "fig, axes = plt.subplots(\n",
    "    1, len(radii), figsize=(15,4), subplot_kw={'projection': '3d'})\n",
    "\n",
    "for radius, ax in zip(radii, axes): \n",
    "    start_time = time.time()\n",
    "    peaks, labels = mean_shift_opt(data, radius)\n",
    "    plot_3d_clusters(ax, data, labels, peaks)\n",
    "    duration = time.time()-start_time\n",
    "    ax.set_title(\n",
    "        f'Found {len(peaks)} peaks using radius={radius:.2f}\\n'\n",
    "        f'Computation took {duration:.4f} s\\n')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **second speedup** is based on a similar principle: Associate points within a distance $\\mathtt{radius}/c$ of the search path with the converged peak ($c$ is some constant value). Use $c = 3$ for this assignment.\n",
    "\n",
    "To realize this speedup, you will need to modify `find_peak` into `find_peak_opt`, which returns two values: `peak` and `is_near_search_path`. The latter should be a Boolean output vector of length $p$ (number of data points) containing `True` for each point that we encountered within a distance $\\mathtt{radius}/c$ on our search path, and `False` for the others. Then use this boolean vector in a new version of `mean_shift_opt`, called `mean_shift_opt2` to do the label assignments accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-375afff9bffe7491",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_peak_opt(data, query, radius, c=3):\n",
    "    is_near_search_path = np.zeros(len(data), dtype=bool)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    while True:\n",
    "        dist = np.linalg.norm(data - query, axis=1)\n",
    "        query_prev = query\n",
    "        query = np.mean(data[dist < radius], axis=0)\n",
    "        # SPEEDUP 2:\n",
    "        is_near_search_path[dist <= radius/c] = True\n",
    "        if np.linalg.norm(query - query_prev) < 0.1:\n",
    "            return query, is_near_search_path\n",
    "    ### END SOLUTION\n",
    "\n",
    "def mean_shift_opt2(data, radius):\n",
    "    labels = np.full(len(data), fill_value=-1, dtype=int)\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    peaks = np.empty(data.shape)\n",
    "    n_peaks = 0\n",
    "    \n",
    "    for idx, query in enumerate(data):\n",
    "        # Skip point if it already has a valid label assigned\n",
    "        if labels[idx] != -1:\n",
    "            continue\n",
    "            \n",
    "        peak, is_near_search_path = find_peak_opt(data, query, radius)\n",
    "        label = None\n",
    "        \n",
    "        # Compare found peak to existing peaks\n",
    "        if n_peaks > 0:\n",
    "            dist = np.linalg.norm(peaks[:n_peaks] - peak, axis=1)\n",
    "            label_of_nearest_peak = np.argmin(dist)\n",
    "            \n",
    "            # If the nearest existing peak is near enough, take its label\n",
    "            if dist[label_of_nearest_peak] <= radius / 2:\n",
    "                label = label_of_nearest_peak\n",
    "        \n",
    "        # No existing peak was near enough, create new one\n",
    "        if label is None:\n",
    "            label = n_peaks\n",
    "            peaks[label] = peak\n",
    "            n_peaks += 1\n",
    "            \n",
    "            # SPEEDUP 1: give same label to points near the peak\n",
    "            dist = np.linalg.norm(data - peak, axis=1)\n",
    "            labels[dist < radius] = label\n",
    "            \n",
    "        # SPEEDUP 2: give same label to points that were near the search path\n",
    "        labels[is_near_search_path] = label\n",
    "    \n",
    "    peaks = peaks[:n_peaks]\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    return peaks, labels\n",
    "\n",
    "data = np.genfromtxt(f'gaussian_mixture_samples_3d.csv', delimiter=',')\n",
    "radii = [1, 1.25, 2, 8]\n",
    "fig, axes = plt.subplots(\n",
    "    1, len(radii), figsize=(15,4), subplot_kw={'projection': '3d'})\n",
    "\n",
    "for radius, ax in zip(radii, axes):\n",
    "    start_time = time.time()\n",
    "    peaks, labels = mean_shift_opt2(data, radius)\n",
    "    plot_3d_clusters(ax, data, labels, peaks)\n",
    "    duration = time.time()-start_time\n",
    "    ax.set_title(f'Found {len(peaks)} peaks using radius={radius:.2f}\\n'\n",
    "                 f'Computation took {duration:.4f} s\\n')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Which radius gives good results and how can one find it?\n",
    "2. How much faster is the optimized version? Does it change the result? If yes, is the result worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1cff4f2c453b1850",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation by Mean-Shift\n",
    "\n",
    "Now use your mean-shift implementation from above to segment images. Note that although very efficient mean-shift implementations exist, our version here may take several minutes per image. Debug using small images.\n",
    "\n",
    "Implement the funtion `mean_shift_segment(im, radius)` where `im` is an input RGB image of shape $\\mathtt{height}\\times\\mathtt{width}\\times 3$ and `radius` is the parameter associated with the mean-shift algorithm. The output should be `data`, `peaks`, `labels`, `segmented_image`:\n",
    "\n",
    "* `data` is an array of the data points that you input to the mean-shift algorithm, with $p$ rows and 3 columns.\n",
    "* `peaks` and `labels` are simply the results returned by the `mean_shift` call (without changing them).\n",
    "* The `segmented_image` is constructed by assigning to each pixel the color value of the corresponding cluster's peak.\n",
    "\n",
    "You will need to reshape (`np.reshape`) the image array before feeding it to your mean-shift clustering function. Also, do not forget to convert the pixel values to floating point, using `somearray.astype(float)` and then convert the result back to 8-bit unsigned integers using `somearray.astype(np.uint8)`.\n",
    "\n",
    "Segment the image `terrain_small.png` using `radius` 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c18d7756305d4bcf",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mean_shift_segment(im, radius):\n",
    "    ### BEGIN SOLUTION\n",
    "    data = im.reshape(-1, 3).astype(float)\n",
    "    peaks, labels = mean_shift_tf(data, radius)  # mean_shift(data, radius)\n",
    "    segmented_im = peaks[labels].reshape(im.shape).astype(np.uint8)\n",
    "    ### END SOLUTION\n",
    "    return data, peaks, labels, segmented_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_label_colormap():\n",
    "    \"\"\"Create a color map for visualizing the labels themselves,\n",
    "    such that the segment boundaries become more visible, unlike\n",
    "    in the visualization using the cluster peak colors.\n",
    "    \"\"\"\n",
    "    import matplotlib.colors\n",
    "    rng = np.random.RandomState(2)\n",
    "    values = np.linspace(0, 1, 20)\n",
    "    colors = plt.cm.get_cmap('hsv')(values)\n",
    "    rng.shuffle(colors)\n",
    "    return matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "label_cmap = make_label_colormap()\n",
    "\n",
    "im = imageio.imread('terrain_small.png')\n",
    "start_time = time.time()\n",
    "data, peaks, labels, segmented_im = mean_shift_segment(im, radius=15)\n",
    "duration= time.time()-start_time\n",
    "print(f'Took {duration:.2f} s')\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "ax.set_title('Original Image')\n",
    "ax.imshow(im)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "ax.set_title('Segmented')\n",
    "ax.imshow(segmented_im)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "ax.set_title('Labels')\n",
    "ax.imshow(labels.reshape(im.shape[:2]), cmap=label_cmap)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "ax.set_title(f'RGB space')\n",
    "plot_3d_clusters(ax, data, labels, peaks, colors='rgb', axis_names='RGB')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation in LUV color space\n",
    "Note that Mean-Shift uses the Euclidean distance metric. Unfortunately, the Euclidean distance in RGB color space does not correlate well to perceived difference in color by people. For example, in the green portion of RGB space, large distances are perceived as the same color, whereas in the blue part a small RGB-distance may represent a large change in perceived color. For this reason you should use the non-linear LUV color space. In this space Euclidean distance better models the perceived difference in color.\n",
    "\n",
    "In the function `mean_shift_segment_luv(...)` cluster the image data in LUV color space by first converting the RGB color vectors to LUV using OpenCV: `cv2.cvtColor(rgb_image, cv2.COLOR_RGB2LUV)`. Then convert the resulting cluster centers back to RGB using `cv2.cvtColor(luv_image, cv2.COLOR_LUV2RGB)`.\n",
    "\n",
    "If we want to include spatial *position information* in the feature vectors as well, we can make the feature vectors 5 dimensional by specifying the LUV values as well as the x,y coordinates of each pixel. Write a function `mean_shift_segment_luv_pos(im, radius)` that does this. **Hint:** useful functions are `np.arange`, `np.meshgrid`, `np.concatenate`, `np.expand_dims`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d0ae0d08f2d3c2c0",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mean_shift_segment_luv(im, radius):\n",
    "    ### BEGIN SOLUTION\n",
    "    im_luv = cv2.cvtColor(im, cv2.COLOR_RGB2LUV)\n",
    "    data = im_luv.reshape(-1, 3).astype(float)\n",
    "    \n",
    "    peaks, labels = mean_shift_tf(data, radius)\n",
    "    \n",
    "    segmented_im = peaks[labels].reshape(im.shape).astype(np.uint8)\n",
    "    segmented_im = cv2.cvtColor(segmented_im, cv2.COLOR_LUV2RGB)\n",
    "    ### END SOLUTION\n",
    "    return data, peaks, labels, segmented_im\n",
    "    \n",
    "def mean_shift_segment_luv_pos(im, radius, pos_factor=1):\n",
    "    ### BEGIN SOLUTION\n",
    "    im_luv = cv2.cvtColor(im, cv2.COLOR_RGB2LUV)\n",
    "    x_range = np.arange(im.shape[1])\n",
    "    y_range = np.arange(im.shape[0])\n",
    "    x, y = np.meshgrid(x_range, y_range)\n",
    "    xy = np.stack([x, y], axis=-1)\n",
    "    feature_im = np.concatenate([im_luv, pos_factor*xy], axis=-1)\n",
    "    data = feature_im.reshape(-1, 5).astype(float)\n",
    "    \n",
    "    peaks, labels = mean_shift_tf(data, radius)\n",
    "    \n",
    "    segmented_im = peaks[labels][..., :3].reshape(im.shape).astype(np.uint8)\n",
    "    segmented_im = cv2.cvtColor(segmented_im, cv2.COLOR_LUV2RGB)\n",
    "    ### END SOLUTION\n",
    "    return data, peaks, labels, segmented_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "im = imageio.imread('terrain_small.png')\n",
    "data, peaks, labels, segmented_im = mean_shift_segment_luv(im, radius=10)\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "ax = fig.add_subplot(2, 3, 1)\n",
    "ax.set_title('Segmented (LUV)')\n",
    "ax.imshow(segmented_im)\n",
    "\n",
    "ax = fig.add_subplot(2, 3, 2)\n",
    "ax.set_title('Labels (LUV)')\n",
    "ax.imshow(labels.reshape(im.shape[:2]), cmap=label_cmap)\n",
    "\n",
    "ax = fig.add_subplot(2, 3, 3, projection='3d')\n",
    "ax.set_title(f'LUV space')\n",
    "plot_3d_clusters(ax, data, labels, peaks, colors='luv', axis_names='LUV')\n",
    "\n",
    "data, peaks, labels, segmented_im = mean_shift_segment_luv_pos(im, radius=20)\n",
    "ax = fig.add_subplot(2, 3, 4)\n",
    "ax.set_title('Segmented (LUV+pos)')\n",
    "ax.imshow(segmented_im)\n",
    "\n",
    "ax = fig.add_subplot(2, 3, 5)\n",
    "ax.set_title('Labels (LUV+pos)')\n",
    "ax.imshow(labels.reshape(im.shape[:2]), cmap=label_cmap)\n",
    "\n",
    "ax = fig.add_subplot(2, 3, 6, projection='3d')\n",
    "ax.set_title(f'VXY space')\n",
    "plot_3d_clusters(\n",
    "    ax, data[:, 2:], labels, peaks[:, 2:], \n",
    "    peak_colors=peaks[:, :3], colors='luv', axis_names='VXY')\n",
    "ax.invert_zaxis()\n",
    "ax.view_init(azim=20, elev=15)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with the parameters\n",
    "\n",
    "How does the `radius` and the *feature vector* affect the resulting segmentations? What effect does adding position information have? What are the advantages and disadvantages of using each type of feature vector? Can you suggest any extensions that might avoid many of the situations where single regions are over-segmented into multiple regions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-427095855283067d",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "radii = [5, 10, 20]\n",
    "fig, axes = plt.subplots(len(radii), 3, figsize=(15, 15))\n",
    "for radius, ax in zip(radii, axes):\n",
    "    segmented_im = mean_shift_segment(im, radius)[-1]\n",
    "    ax[0].imshow(segmented_im)\n",
    "    ax[0].set_title(f'Radius {radius} RGB')\n",
    "    \n",
    "    segmented_im = mean_shift_segment_luv(im, radius)[-1]\n",
    "    ax[1].imshow(segmented_im)\n",
    "    ax[1].set_title(f'Radius {radius} LUV')\n",
    "\n",
    "    segmented_im = mean_shift_segment_luv_pos(im, radius)[-1]\n",
    "    ax[2].imshow(segmented_im)\n",
    "    ax[2].set_title(f'Radius {radius} LUV+pos')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [BONUS] Test it on a larger image\n",
    "\n",
    "Run your algorithm on at least one larger (approx. 300x200) test image using your own choice of `radius` and feature vector definition. One source for possible test images is the dataset of images available at http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/. You can also try the example images included in the scikit-image library, e.g. `import skimage.data; im = skimage.data.astronaut()`. Or any image from anywhere.\n",
    "\n",
    "Processing can take several minutes for larger images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-22b5011e13b8557a",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import skimage.data\n",
    "im = skimage.data.astronaut()\n",
    "im = cv2.resize(im, (256,256))\n",
    "\n",
    "im = imageio.imread('palace.jpg')\n",
    "im = cv2.resize(im, (256,169))\n",
    "\n",
    "im = imageio.imread('toucan.jpg')\n",
    "im = cv2.resize(im, (256,256))\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "start_time = time.time()\n",
    "data, peaks, labels, segmented_im = mean_shift_segment_luv_pos(im, radius=15)\n",
    "duration = time.time()-start_time\n",
    "print(f'Took {duration:.2f} seconds.')\n",
    "### END SOLUTION\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].set_title('Original image')\n",
    "axes[0].imshow(im)\n",
    "axes[1].set_title('Segmented image')\n",
    "axes[1].imshow(segmented_im)\n",
    "axes[2].set_title('Labels')\n",
    "axes[2].imshow(labels.reshape(im.shape[:2]), cmap=label_cmap)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [BONUS++] Efficient Implementation\n",
    "\n",
    "Mean-shift is highly parallelizable and GPU-based implementations can be many times faster for such workloads.\n",
    "If you already know TensorFlow or CUDA, you can try implementing mean-shift for the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-684ee995b5053e86",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MeanShiftTF:\n",
    "    # All TensorFlow tensors are suffixed with _t for clarity.\n",
    "    def __init__(self):\n",
    "        \"\"\"Build TensorFlow graph for peak-finding and \n",
    "        initialize a session.\"\"\"\n",
    "        \n",
    "        self.data_t = tf.placeholder(shape=(None, None), dtype=tf.float32)\n",
    "        self.radius_t = tf.placeholder(shape=(), dtype=tf.float32)\n",
    "        self.query_indices_t = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "        query_t = tf.gather(self.data_t, self.query_indices_t)\n",
    "        is_near_path_shape = (tf.shape(query_t)[0], tf.shape(self.data_t)[0])\n",
    "        self.is_near_path_init_t = tf.zeros(shape=is_near_path_shape, dtype=tf.bool)\n",
    "        \n",
    "        self.converged_query_t, _, self.is_near_path_t = tf.while_loop(\n",
    "            cond=self.has_not_converged_t, \n",
    "            body=self.mean_shift_step_t,\n",
    "            loop_vars=(query_t, np.float32(np.inf), self.is_near_path_init_t))\n",
    "        \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        \n",
    "    def has_not_converged_t(self, query_t, max_shift_t, is_near_path_t):\n",
    "        return max_shift_t > np.float32(0.1)\n",
    "    \n",
    "    def mean_shift_step_t(self, query_t, max_shift_t, is_near_path_t):\n",
    "        expanded_query_t = tf.expand_dims(query_t, 2) # shape [m, d, 1]\n",
    "        expanded_data_t = tf.transpose(\n",
    "            tf.expand_dims(self.data_t, 2), [2,1,0]) # shape [1, d, n]\n",
    "        expanded_data_t = expanded_data_t\n",
    "        expanded_query_t = expanded_query_t\n",
    "        offsets_t = expanded_data_t - expanded_query_t\n",
    "        dist_t = tf.norm(offsets_t, axis=1, keepdims=True) # shape [m, n, 1]\n",
    "\n",
    "        is_within_radius_t = (\n",
    "            tf.cast(dist_t <= self.radius_t, query_t.dtype) + \n",
    "            tf.zeros_like(expanded_data_t)) # shape [m, d, n]\n",
    "            \n",
    "        num_within_radius_t = tf.reduce_sum(\n",
    "            is_within_radius_t, axis=2)  # shape [m, d]\n",
    "        \n",
    "        offset_sum_within_radius_t = tf.reduce_sum(\n",
    "            is_within_radius_t * offsets_t, axis=2)  # shape [m, d]\n",
    "        \n",
    "        shift_t = offset_sum_within_radius_t / num_within_radius_t\n",
    "        max_shift_t = tf.reduce_max(tf.norm(shift_t, axis=1))\n",
    "        \n",
    "        is_near_path_t = tf.logical_or(\n",
    "            is_near_path_t,\n",
    "            tf.squeeze(dist_t, axis=1) <= self.radius_t / 3) # shape [m, n]\n",
    "        return query_t + shift_t, max_shift_t, is_near_path_t\n",
    "    \n",
    "    def apply(self, data, radius, chunk_size=6):\n",
    "        \"\"\"Apply mean-shift clustering on the NumPy array `data`.\"\"\"\n",
    "        \n",
    "        labels = np.full(len(data), fill_value=-1, dtype=int)\n",
    "        peaks = np.empty_like(data)\n",
    "        thres_sq = (radius/2) ** 2\n",
    "        n_peaks = 0\n",
    "       \n",
    "        while np.any(labels==-1):\n",
    "            query_indices = np.argwhere(labels==-1)[:chunk_size,0]\n",
    "            \n",
    "            # This is where the bulk of the computation happens\n",
    "            converged_queries, is_near_paths = self.sess.run(\n",
    "                [self.converged_query_t, self.is_near_path_t], \n",
    "                feed_dict={\n",
    "                    self.data_t: data.astype(np.float32),\n",
    "                    self.radius_t: np.float32(radius),\n",
    "                    self.query_indices_t: query_indices})\n",
    "            \n",
    "            # This part is basically the same as in the CPU-based function\n",
    "            for idx, converged_query, is_near_path in zip(\n",
    "                    query_indices, converged_queries, is_near_paths):\n",
    "                label = None\n",
    "\n",
    "                # Compare found peak to existing peaks\n",
    "                if n_peaks > 0:\n",
    "                    dist = np.linalg.norm(peaks[:n_peaks] - converged_query, axis=1)\n",
    "                    label_of_nearest_peak = np.argmin(dist)\n",
    "\n",
    "                    # If the nearest existing peak is near enough, take its label\n",
    "                    if dist[label_of_nearest_peak] <= radius / 2:\n",
    "                        label = label_of_nearest_peak\n",
    "\n",
    "                # No existing peak was near enough, create new one\n",
    "                if label is None:\n",
    "                    label = n_peaks\n",
    "                    peaks[label] = converged_query\n",
    "                    n_peaks += 1\n",
    "\n",
    "                    # SPEEDUP 1: give same label to points near the peak\n",
    "                    dist = np.linalg.norm(data - converged_query, axis=1)\n",
    "                    labels[dist <= radius] = label\n",
    "\n",
    "                # SPEEDUP 2: give same label to points near search path\n",
    "                labels[is_near_path] = label\n",
    "\n",
    "        return peaks[:n_peaks], labels\n",
    "        \n",
    "mean_shift_tf = MeanShiftTF().apply\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
